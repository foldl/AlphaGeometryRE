{
    "bluelm": {
        "brief": "BlueLM is a large-scale open-source language model independently developed by the vivo AI Lab. This release includes 2K and 32K context length versions for both Base and Chat models.",
        "default": "7b",
        "license": "https://github.com/vivo-ai-lab/BlueLM/blob/main/MODEL_LICENSE.pdf",
        "variants": {
            "7b": {
                "default": "q8",
                "quantized": {
                    "q8": {
                        "size": 7753748688,
                        "url": "chatllm_quantized_bluelm/bluelm-7b.bin"
                    }
                }
            },
            "7b-32k": {
                "default": "q8",
                "quantized": {
                    "q8": {
                        "size": 7754514672,
                        "url": "chatllm_quantized_bluelm/bluelm-7b-32k.bin"
                    }
                }
            }
        }
    },
    "llama3.1": {
        "brief": "Llama 3.1 is a new state-of-the-art model from Meta available in 8B, 70B and 405B parameter sizes.",
        "default": "8b",
        "license": "LLAMA 3.1 COMMUNITY LICENSE AGREEMENT",
        "variants": {
            "8b": {
                "default": "q4_1",
                "quantized": {
                    "q4_1": {
                        "size": 5025629376,
                        "url": "chatllm_quantized_240726/llama3.1-8b_q4_1.bin"
                    },
                    "q8": {
                        "size": 8538752192,
                        "url": "chatllm_quantized_240726/llama3.1-8b.bin"
                    }
                }
            }
        }
    },
    "llama3-groq-tool-use": {
        "brief": "Llama 3 Groq Tool Use model, specifically designed for advanced tool use and function calling tasks.",
        "default": "8b",
        "license": "LLAMA 3.1 COMMUNITY LICENSE AGREEMENT",
        "variants": {
            "8b": {
                "default": "q8",
                "quantized": {
                    "q8": {
                        "size": 8538804528,
                        "url": "chatllm_quantized_240726/llama3-groq-tool-8b.bin"
                    }
                }
            }
        }
    },
    "mistral-nemo": {
        "brief": "Mistral NeMo, a 12B model that offers a large context window of up to 128k tokens.",
        "default": "12b",
        "license": "Apache License Version 2.0",
        "variants": {
            "12b": {
                "default": "q4_1",
                "quantized": {
                    "q4_1": {
                        "size": 7662149840,
                        "url": "chatllm_quantized_240722/nemo-12b-q4_1.bin"
                    },
                    "q8": {
                        "size": 13020373200,
                        "url": "chatllm_quantized_240722/nemo-12b.bin"
                    }
                }
            }
        }
    },
    "smollm": {
        "brief": "SmolLM, a family of state-of-the-art small models with 135M, 360M, and 1.7B parameters, trained on a new high-quality dataset.",
        "default": "1.7b",
        "license": "Apache License Version 2.0",
        "variants": {
            "12b": {
                "default": "q8",
                "quantized": {
                    "q8": {
                        "size": 1819874192,
                        "url": "chatllm_quantized_240722/smolllm-1.7b.bin"
                    }
                }
            }
        }
    },
    "zhinao": {
        "brief": "360Zhinao from Qihoo360.",
        "default": "7b-4k",
        "license": "Apache License Version 2.0",
        "variants": {
            "7b-4k": {
                "default": "q4_1",
                "quantized": {
                    "q4_1": {
                        "size": 4865497088,
                        "url": "chatllm_quantized_zhinao/zhinao-7b-4k-q4_1.bin"
                    }
                }
            },
            "7b-360k": {
                "default": "q4_1",
                "quantized": {
                    "q4_1": {
                        "size": 4865497088,
                        "url": "chatllm_quantized_zhinao/zhinao-7b-360k-q4_1.bin"
                    }
                }
            }
        }
    },
    "mistral": {
        "brief": "The Mistral-7B-Instruct-v0.3 Large Language Model (LLM) is an instruct fine-tuned version of the Mistral-7B-v0.3.",
        "default": "7b",
        "license": "Apache License Version 2.0",
        "variants": {
            "7b": {
                "default": "q8",
                "quantized": {
                    "q8": {
                        "size": 7702259552,
                        "url": "chatllm_quantized_mistral/mistral-7b-v0.3.bin"
                    }
                }
            }
        }
    },
    "numinamath": {
        "brief": "NuminaMath is a series of language models that are trained to solve math problems using tool-integrated reasoning (TIR).",
        "default": "7b",
        "license": "Apache License Version 2.0",
        "variants": {
            "7b": {
                "default": "q8",
                "quantized": {
                    "q8": {
                        "size": 7345830096,
                        "url": "chatllm_quantized_numinamath/numinamath.bin"
                    },
                    "f16": {
                        "size": 13824063696,
                        "url": "chatllm_quantized_numinamath/numinamath-f16.bin"
                    }
                }
            }
        }
    },
    "codegeex4": {
        "brief": "CodeGeeX4: Open Multilingual Code Generation Model.",
        "default": "9b",
        "license": "https://github.com/THUDM/CodeGeeX4/blob/main/MODEL_LICENSE",
        "variants": {
            "9b": {
                "default": "q4_1",
                "quantized": {
                    "q4_1": {
                        "size": 5881053648,
                        "url": "chatllm_quantized_codegeex4/codegeex4-all-9b-q4_1.bin"
                    },
                    "q8": {
                        "size": 9993306576,
                        "url": "chatllm_quantized_codegeex4/codegeex4-all-9b.bin"
                    }
                }
            }
        }
    },
    "internlm2.5": {
        "brief": "InternLM2.5 is a 7B parameter model tailored for practical scenarios with outstanding reasoning capability.",
        "default": "1.8b",
        "license": "Apache License Version 2.0",
        "variants": {
            "1.8b": {
                "default": "q8",
                "quantized": {
                    "q8": {
                        "size": 2008808560,
                        "url": "chatllm_quantized_internlm2.5_1.8b/internlm2.5-1.8b.bin"
                    }
                }
            },
            "7b": {
                "default": "q8",
                "quantized": {
                    "q8": {
                        "size": 8223436400,
                        "url": "chatllm_quantized_internlm/internlm2.5-7b.bin"
                    }
                }
            },
            "20b": {
                "default": "q4_0",
                "quantized": {
                    "q4_0": {
                        "size": 11175293552,
                        "url": "chatllm_quantized_internlm2.5_1.8b/internlm2.5-20b-q4_0.bin"
                    }
                }
            },
            "7b-1m": {
                "default": "q8",
                "quantized": {
                    "q8": {
                        "size": 8223436400,
                        "url": "chatllm_quantized_internlm/internlm2.5-7b-1m.bin"
                    }
                }
            }
        }
    },
    "internlm2": {
        "brief": "The second generation of the InternLM model.",
        "default": "1.8b",
        "license": "https://huggingface.co/internlm/internlm2-7b#open-source-license",
        "variants": {
            "1.8b": {
                "default": "q8",
                "quantized": {
                    "q8": {
                        "size": 2008808560,
                        "url": "chatllm_quantized_internlm/internlm2-1.8B.bin"
                    }
                }
            },
            "8b": {
                "default": "q8",
                "quantized": {
                    "q8": {
                        "size": 8223436400,
                        "url": "chatllm_quantized_internlm/internlm2-chat-8b-new.bin"
                    }
                }
            },
            "20b": {
                "default": "q8",
                "quantized": {
                    "q8": {
                        "size": 21105570416,
                        "url": "chatllm_quantized_internlm/internlm2-20b.bin"
                    }
                }
            }
        }
    },
    "internlm2-math": {
        "brief": "State-of-the-art bilingual open-sourced Math reasoning LLMs. A solver, prover, verifier, augmenter.",
        "default": "1.8b",
        "license": "Unknown. See https://huggingface.co/internlm/internlm2-math-plus-1_8b",
        "variants": {
            "1.8b": {
                "default": "q8",
                "quantized": {
                    "q8": {
                        "size": 2008808560,
                        "url": "chatllm_quantized_internlm/internlm2-math-plus-1_8b.bin"
                    }
                }
            }
        }
    },
    "internlm1": {
        "brief": "InternLM has open-sourced a 7 billion parameter base model and a chat model tailored for practical scenarios.",
        "default": "1.8b",
        "license": "https://huggingface.co/internlm/internlm-chat-7b#open-source-license",
        "variants": {
            "1.8b": {
                "default": "q8",
                "quantized": {
                    "q8": {
                        "size": 7783400944,
                        "url": "chatllm_quantized_internlm/internlmv1.1_7b.bin"
                    }
                }
            }
        }
    },
    "llm-compiler": {
        "brief": "LLM Compiler is a state-of-the-art LLM that builds upon Code Llama with improved performance for code optimization and compiler reasoning.",
        "default": "7b",
        "license": "https://huggingface.co/facebook/llm-compiler-7b/blob/main/LICENSE.pdf",
        "variants": {
            "7b": {
                "default": "q8",
                "quantized": {
                    "q8": {
                        "size": 7160799888,
                        "url": "chatllm_quantized_llm-compiler/llm-compiler-7b.bin"
                    }
                }
            }
        }
    },
    "index": {
        "brief": "LLM developed by Bilibili.",
        "default": "1.9b-chat",
        "license": "https://huggingface.co/IndexTeam/Index-1.9B-Chat/blob/main/LICENSE",
        "variants": {
            "1.9b-chat": {
                "default": "q8",
                "quantized": {
                    "q8": {
                        "size": 2309982912,
                        "url": "chatllm_quantized_index/index.bin"
                    }
                }
            },
            "1.9b-character": {
                "default": "q8",
                "quantized": {
                    "q8": {
                        "size": 2309982912,
                        "url": "chatllm_quantized_index/index-ch.bin"
                    }
                }
            }
        }
    },
    "glm-4": {
        "brief": "GLM-4-9B is the open-source version of the latest generation of pre-trained models in the GLM-4 series launched by Zhipu AI.",
        "default": "9b",
        "license": "https://huggingface.co/THUDM/glm-4-9b/blob/main/LICENSE",
        "variants": {
            "9b": {
                "default": "q8",
                "quantized": {
                    "q8": {
                        "size": 9993306576,
                        "url": "chatllm_quantized_glm/glm4.bin"
                    }
                }
            }
        }
    },
    "chatglm3": {
        "brief": "ChatGLM3 is a generation of pre-trained dialogue models jointly released by Zhipu AI and Tsinghua KEG.",
        "default": "6b",
        "license": "https://github.com/THUDM/ChatGLM3/blob/main/MODEL_LICENSE",
        "variants": {
            "6b": {
                "default": "q8",
                "quantized": {
                    "q8": {
                        "size": 6635798992,
                        "url": "chatllm_quantized_glm/chatglm3-6b.bin"
                    }
                }
            }
        }
    },
    "chatglm2": {
        "brief": "ChatGLM2-6B is the second-generation version of the open-source bilingual (Chinese-English) chat model ChatGLM-6B.",
        "default": "6b",
        "license": "https://github.com/THUDM/ChatGLM2-6B/blob/main/MODEL_LICENSE",
        "variants": {
            "6b": {
                "default": "q8",
                "quantized": {
                    "q8": {
                        "size": 6635798992,
                        "url": "chatllm_quantized_glm/chatglm2.bin"
                    }
                }
            }
        }
    },
    "phi3": {
        "brief": "Phi-3 is a family of lightweight 3B (Mini) and 14B (Medium) state-of-the-art open models by Microsoft.",
        "default": "mini-4k",
        "license": "MIT",
        "variants": {
            "mini-4k": {
                "default": "q8",
                "quantized": {
                    "q8": {
                        "size": 4060935568,
                        "url": "chatllm_quantized_phi3/phi3-mini-4k.bin"
                    }
                }
            },
            "mini-128k": {
                "default": "q8",
                "quantized": {
                    "q8": {
                        "size": 4060936592,
                        "url": "chatllm_quantized_phi3/phi3-mini-128k.bin"
                    }
                }
            },
            "medium-4k": {
                "default": "q4_1",
                "quantized": {
                    "q4_1": {
                        "size": 8727004560,
                        "url": "chatllm_quantized_phi3-medium-4k/phi3-medium-4k_q4_1.bin"
                    },
                    "q8": {
                        "size": 14834427280,
                        "url": "chatllm_quantized_phi3-medium-4k/phi3-medium-4k.bin"
                    }
                }
            },
            "medium-128k": {
                "default": "q4_1",
                "quantized": {
                    "q4_1": {
                        "size": 8727005584,
                        "url": "chatllm_quantized_phi3/phi3-medium-128k_q4_1.bin"
                    }
                }
            }
        }
    },
    "phi3.5": {
        "brief": "Phi-3.5-mini enhances multi-lingual support with a 128K context length. Phi-3.5-MoE, featuring 16 experts and 6.6B active parameters.",
        "default": "mini",
        "license": "MIT",
        "variants": {
            "mini": {
                "default": "q8",
                "quantized": {
                    "q8": {
                        "size": 4060935840,
                        "url": "chatllm_quantized_phi-3.5/phi3.5-mini.bin"
                    }
                }
            },
            "moe": {
                "default": "q4_1",
                "quantized": {
                    "q4_1": {
                        "size": 26174315504,
                        "url": "chatllm_quantized_phi-3.5/phi3.5-moe_q4_1.bin"
                    }
                }
            }
        }
    },
    "yi-1.5": {
        "brief": "Yi 1.5 is a high-performing, bilingual language model.",
        "default": "6b",
        "license": "Apache License Version 2.0",
        "variants": {
            "6b": {
                "default": "q8",
                "quantized": {
                    "q8": {
                        "size": 6441564880,
                        "url": "chatllm_quantized_yi1.5/yi1.5-6b.bin"
                    }
                }
            },
            "9b": {
                "default": "q8",
                "quantized": {
                    "q8": {
                        "size": 9383354064,
                        "url": "chatllm_quantized_yi1.5/yi1.5-9b.bin"
                    }
                }
            },
            "9b-16k": {
                "default": "q4_1",
                "quantized": {
                    "q4_1": {
                        "size": 5520662224,
                        "url": "chatllm_quantized_yi1.5/yi1.5-9b-16k_q4_1.bin"
                    }
                }
            }
        }
    },
    "yi-coder": {
        "brief": "Yi-Coder is a series of open-source code language models that delivers state-of-the-art coding performance with fewer than 10 billion parameters.",
        "default": "1.5b",
        "license": "Apache License Version 2.0",
        "variants": {
            "1.5b": {
                "default": "q8",
                "quantized": {
                    "q8": {
                        "size": 1569999056,
                        "url": "chatllm_quantized_20240916/yi-coder-1.5B-chat.bin"
                    }
                }
            }
        }
    },
    "deepseek-v2": {
        "brief": "A strong, economical, and efficient Mixture-of-Experts language model.",
        "default": "light",
        "license": "DEEPSEEK LICENSE AGREEMENT Version 1.0, 23 October 2023",
        "variants": {
            "light": {
                "default": "q8",
                "quantized": {
                    "q8": {
                        "size": 16691737872,
                        "url": "chatllm_quantized_deepseek/deepseekv2-lite.bin"
                    }
                }
            }
        }
    },
    "deepseek-coder-v2": {
        "brief": "An open-source Mixture-of-Experts code language model that achieves performance comparable to GPT4-Turbo in code-specific tasks.",
        "default": "light",
        "license": "DEEPSEEK LICENSE AGREEMENT Version 1.0, 23 October 2023",
        "variants": {
            "light": {
                "default": "q4_1",
                "quantized": {
                    "q8": {
                        "size": 16691738304,
                        "url": "chatllm_quantized_deepseek/deepseek-coder-v2-lite-instruct.bin"
                    },
                    "q4_1": {
                        "size": 9820206784,
                        "url": "chatllm_quantized_deepseek/deepseek-coder-v2-lite-instruct_q4_1.bin"
                    }
                }
            }
        }
    },
    "deepseek-coder-v2-base": {
        "brief": "An open-source Mixture-of-Experts code language model that achieves performance comparable to GPT4-Turbo in code-specific tasks.",
        "default": "light",
        "license": "DEEPSEEK LICENSE AGREEMENT Version 1.0, 23 October 2023",
        "variants": {
            "light": {
                "default": "q4_1",
                "quantized": {
                    "q4_1": {
                        "size": 9820206784,
                        "url": "chatllm_quantized_deepseek/deepseek-coder-v2-lite-base_q4_1.bin"
                    }
                }
            }
        }
    },
    "deepseek-llm": {
        "brief": "An advanced language model crafted with 2 trillion bilingual tokens.",
        "default": "7b",
        "license": "DEEPSEEK LICENSE AGREEMENT Version 1.0, 23 October 2023",
        "variants": {
            "7b": {
                "default": "q8",
                "quantized": {
                    "q8": {
                        "size": 7345830176,
                        "url": "chatllm_quantized_deepseek/deepseek-7b.bin"
                    }
                }
            }
        }
    },
    "deepseek-coder": {
        "brief": "DeepSeek Coder is a capable coding model trained on two trillion code and natural language tokens.",
        "default": "1.3b",
        "license": "DEEPSEEK LICENSE AGREEMENT Version 1.0, 23 October 2023",
        "variants": {
            "1.3b": {
                "default": "q8",
                "quantized": {
                    "q8": {
                        "size": 1431733904,
                        "url": "chatllm_quantized_deepseek/deepseekcoder-1.3b.bin"
                    }
                }
            },
            "7b": {
                "default": "q8",
                "quantized": {
                    "q8": {
                        "size": 7163394192,
                        "url": "chatllm_quantized_deepseek/deepseekcoder-7b.bin"
                    }
                }
            }
        }
    },
    "deepseek-coder-base": {
        "brief": "DeepSeek Coder is a capable coding model trained on two trillion code and natural language tokens.",
        "default": "1.3b",
        "license": "DEEPSEEK LICENSE AGREEMENT Version 1.0, 23 October 2023",
        "variants": {
            "1.3b": {
                "default": "q8",
                "quantized": {
                    "q8": {
                        "size": 1431733904,
                        "url": "chatllm_quantized_deepseek/deepseekcoder-1.3b-base.bin"
                    }
                }
            },
            "6.7b": {
                "default": "q8",
                "quantized": {
                    "q8": {
                        "size": 7163394192,
                        "url": "chatllm_quantized_deepseek/deepseekcoder-6.7b-base.bin"
                    }
                }
            }
        }
    },
    "gemma": {
        "brief": "Gemma is a family of lightweight, state-of-the-art open models built by Google DeepMind. Updated to version 1.1.",
        "default": "2b",
        "license": "https://ai.google.dev/gemma/terms",
        "variants": {
            "2b": {
                "default": "q8",
                "quantized": {
                    "q8": {
                        "size": 1570351200,
                        "url": "chatllm_quantized_models/gemma-1.1-2b.bin"
                    }
                }
            }
        }
    },
    "gemma2": {
        "brief": "Gemma is a family of lightweight, state-of-the-art open models built by Google DeepMind. Updated to version 2.",
        "default": "2b",
        "license": "https://ai.google.dev/gemma/terms",
        "variants": {
            "2b": {
                "default": "q8",
                "quantized": {
                    "q8": {
                        "size": 2782194800,
                        "url": "chatllm_quantized_gemma2_2b/gemma2-2b.bin"
                    },
                    "f16": {
                        "size": 5232913520,
                        "url": "chatllm_quantized_gemma2_2b/gemma2-2b-f16.bin"
                    }
                }
            },
            "9b": {
                "default": "q4_1",
                "quantized": {
                    "q4_1": {
                        "size": 5781867888,
                        "url": "chatllm_quantized_gemma-2/gemma-2-9b_q4_1.bin"
                    },
                    "q8": {
                        "size": 9824849264,
                        "url": "chatllm_quantized_gemma-2/gemma-2-9b.bin"
                    }
                }
            },
            "27b": {
                "default": "q4_1",
                "quantized": {
                    "q4_1": {
                        "size": 17023592624,
                        "url": "chatllm_quantized_gemma-2/gemma-2-27b_q4_1.bin"
                    },
                    "q8": {
                        "size": 28935088304,
                        "url": "chatllm_quantized_gemma-2/gemma-2-27b.bin"
                    }
                }
            }
        }
    },
    "llama3": {
        "brief": "Meta Llama 3: The most capable openly available LLM to date.",
        "default": "8b",
        "license": "https://llama.meta.com/llama3/license",
        "variants": {
            "8b": {
                "default": "q4_1",
                "quantized": {
                    "q4_1": {
                        "size": 5025629392,
                        "url": "chatllm_quantized_llama3/llama3-8b-q4_1.bin"
                    },
                    "q8": {
                        "size": 8538752208,
                        "url": "chatllm_quantized_llama3/llama3-8b.bin"
                    }
                }
            }
        }
    },
    "llama3-chinese-lora": {
        "brief": "Llama-3-Chinese-8B-Instruct-LoRA, which is further tuned with 5M instruction data on Llama-3-Chinese-8B.",
        "default": "8b",
        "license": "",
        "variants": {
            "8b": {
                "default": "q4_0",
                "quantized": {
                    "q4_0": {
                        "size": 4523754704,
                        "url": "chatllm_quantized_llama3/llama3-8b-lora-q4_0.bin"
                    }
                }
            }
        }
    },
    "minicpm": {
        "brief": "MiniCPM is an End-Size LLM developed by ModelBest Inc. and TsinghuaNLP, with only 2.4B parameters excluding embeddings.",
        "default": "2b-sft",
        "license": "https://github.com/OpenBMB/General-Model-License/blob/main/%E9%80%9A%E7%94%A8%E6%A8%A1%E5%9E%8B%E8%AE%B8%E5%8F%AF%E5%8D%8F%E8%AE%AE-%E6%9D%A5%E6%BA%90%E8%AF%B4%E6%98%8E-%E5%AE%A3%E4%BC%A0%E9%99%90%E5%88%B6-%E5%95%86%E4%B8%9A%E6%8E%88%E6%9D%83.md",
        "variants": {
            "2b-dpo": {
                "default": "q4_1",
                "quantized": {
                    "q4_1": {
                        "size": 1705488848,
                        "url": "chatllm_quantized_models/minicpm-dpo-q4_1.bin"
                    }
                }
            },
            "2b-sft": {
                "default": "q8",
                "quantized": {
                    "q8": {
                        "size": 2897542592,
                        "url": "chatllm_quantized_models/minicpm_sft_q8.bin"
                    }
                }
            },
            "moe": {
                "default": "q4_1",
                "quantized": {
                    "q4_1": {
                        "size": 8673301984,
                        "url": "chatllm_quantized_minicpm_moe/minicpm-moe-q4_1.bin"
                    }
                }
            }
        }
    },
    "minicpm3": {
        "brief": "MiniCPM3-4B is the 3rd generation of MiniCPM series.",
        "default": "4b",
        "license": "Apache License 2.0",
        "variants": {
            "4b": {
                "default": "q8",
                "quantized": {
                    "q8": {
                        "size": 4330719488,
                        "url": "chatllm_quantized_20240916/minicpm3-4b.bin"
                    }
                }
            }
        }
    },
    "qanything": {
        "brief": "QAnything is a local knowledge base question-answering system based on QwenLM.",
        "default": "7b",
        "license": "Apache License 2.0",
        "variants": {
            "7b": {
                "default": "q4_1",
                "quantized": {
                    "q4_1": {
                        "size": 4832334112,
                        "url": "chatllm_quantized_models/qwen-qany-7b-q4_1.bin"
                    }
                }
            }
        }
    },
    "qwen1.5": {
        "brief": "Qwen1.5 is the beta version of Qwen2 from Alibaba group.",
        "default": "moe",
        "license": "Tongyi Qianwen RESEARCH LICENSE AGREEMENT",
        "variants": {
            "1.8b": {
                "default": "q8",
                "quantized": {
                    "q8": {
                        "size": 1956630800,
                        "url": "chatllm_quantized_models/qwen1.5-1.8b.bin"
                    }
                }
            },
            "moe": {
                "default": "q4_1",
                "quantized": {
                    "q4_1": {
                        "size": 8952812112,
                        "url": "chatllm_quantized_models/qwen1.5-moe-q4_1.bin"
                    }
                }
            }
        }
    },
    "qwen2": {
        "brief": "Qwen2 is a new series of large language models from Alibaba group.",
        "default": "1.5b",
        "license": "All models with the exception of Qwen2 72B (both instruct and base models) are Apache 2.0 licensed. Qwen2 72B model still uses the original Qianwen License.",
        "variants": {
            "0.5b": {
                "default": "q8",
                "quantized": {
                    "q8": {
                        "size": 529392352,
                        "url": "chatllm_quantized_qwen2/qwen2-0.5b.bin"
                    }
                }
            },
            "1.5b": {
                "default": "q8",
                "quantized": {
                    "q8": {
                        "size": 1644897504,
                        "url": "chatllm_quantized_qwen2/qwen2-1.5b.bin"
                    }
                }
            },
            "7b": {
                "default": "q8",
                "quantized": {
                    "q8": {
                        "size": 8096847120,
                        "url": "chatllm_quantized_qwen2/qwen2-7b.bin"
                    }
                }
            }
        }
    },
    "qwen2.5" : {
        "brief": "Qwen2 is a new series of large language models from Alibaba group.",
        "default": "1.5b",
        "license": "All models, except for the 3B (QWen Research) and 72B (Qwen) variants, are licensed under Apache 2.0.",
        "variants": {
            "1.5b": {
                "default": "q8",
                "quantized": {
                    "q8": {
                        "size": 1644897856,
                        "url": "chatllm_quantized_qwen2.5/qwen2.5-1.5b.bin"
                    }
                }
            },
            "7b": {
                "default": "q8",
                "quantized": {
                    "q8": {
                        "size": 8096847472,
                        "url": "chatllm_quantized_qwen2.5/qwen2.5-7b.bin"
                    }
                }
            }
        }
    },
    "qwen2.5-coder" : {
        "brief": "Qwen2 is a new series of large language models from Alibaba group.",
        "default": "1.5b",
        "license": "All models, except for the 3B (QWen Research) and 72B (Qwen) variants, are licensed under Apache 2.0.",
        "variants": {
            "1.5b": {
                "default": "q8",
                "quantized": {
                    "q8": {
                        "size": 1644897856,
                        "url": "chatllm_quantized_qwen2.5/qwen2.5-coder-1.5b.bin"
                    }
                }
            },
            "7b": {
                "default": "q4_0",
                "quantized": {
                    "q8": {
                        "size": 8096847472,
                        "url": "chatllm_quantized_241110/qwen2.5-coder-7b.bin"
                    },
                    "q4_0": {
                        "size": 4289205872,
                        "url": "chatllm_quantized_241110/qwen2.5-coder-7b_q4_0.bin"
                    }
                }
            }
        }
    },
    "qwen2.5.1-coder" : {
        "brief": "Qwen2 is a new series of large language models from Alibaba group.",
        "default": "1.5b",
        "license": "All models, except for the 3B (QWen Research) and 72B (Qwen) variants, are licensed under Apache 2.0.",
        "variants": {
            "1.5b": {
                "default": "q8",
                "quantized": {
                    "q8": {
                        "size": 1644897856,
                        "url": "chatllm_quantized_241110/qwen2.5.1-coder-1.5b.bin"
                    }
                }
            },
            "7b": {
                "default": "q4_0",
                "quantized": {
                    "q8": {
                        "size": 8096847472,
                        "url": "chatllm_quantized_241110/qwen2.5.1-coder-7b.bin"
                    },
                    "q4_0": {
                        "size": 4289205872,
                        "url": "chatllm_quantized_241110/qwen2.5.1-coder-7b_q4_0.bin"
                    }
                }
            }
        }
    },
    "qwen2.5-math" : {
        "brief": "Qwen2 is a new series of large language models from Alibaba group.",
        "default": "7b",
        "license": "All models, except for the 3B (QWen Research) and 72B (Qwen) variants, are licensed under Apache 2.0.",
        "variants": {
            "7b": {
                "default": "q8",
                "quantized": {
                    "q8": {
                        "size": 8096847472,
                        "url": "chatllm_quantized_qwen2.5/qwen2.5-math-7b.bin"
                    }
                }
            }
        }
    },
    "starling-lm": {
        "brief": "Starling is a large language model trained by reinforcement learning from AI feedback focused on improving chatbot helpfulness.",
        "default": "7b",
        "license": "Apache License Version 2.0",
        "variants": {
            "7b": {
                "default": "q4_1",
                "quantized": {
                    "q4_1": {
                        "size": 4074845056,
                        "url": "chatllm_quantized_models/starling-7b-q4_1.bin"
                    }
                }
            }
        }
    },
    "yi-1": {
        "brief": "Yi (v1) is a high-performing, bilingual language model.",
        "default": "34b",
        "license": "Apache License Version 2.0",
        "variants": {
            "34b": {
                "default": "q4_1",
                "quantized": {
                    "q4_1": {
                        "size": 19347696080,
                        "url": "chatllm_quantized_models/yi-34b-q4.bin"
                    }
                }
            }
        }
    },
    "granite3-dense": {
        "brief": "The IBM Granite 2B and 8B models are designed to support tool-based use cases and support for retrieval augmented generation (RAG), streamlining code generation, translation and bug fixing.",
        "default": "2b",
        "license": "Apache License Version 2.0",
        "variants": {
            "2b": {
                "default": "q8",
                "quantized": {
                    "q8": {
                        "size": 2693566832,
                        "url": "chatllm_quantized_granite_3.0/granite-3-2b.bin"
                    }
                }
            },
            "8b": {
                "default": "q8",
                "quantized": {
                    "q8": {
                        "size": 8683703024,
                        "url": "chatllm_quantized_granite_3.0/granite-3-8b.bin"
                    }
                }
            }
        }
    },
    "granite3-moe": {
        "brief": "The IBM Granite 1B and 3B models are the first mixture of experts (MoE) Granite models from IBM designed for low latency usage.",
        "default": "1b",
        "license": "Apache License Version 2.0",
        "variants": {
            "1b": {
                "default": "q8",
                "quantized": {
                    "q8": {
                        "size": 1419564608,
                        "url": "chatllm_quantized_granite_3.0/granite-3-moe-1b.bin"
                    }
                }
            },
            "3b": {
                "default": "q8",
                "quantized": {
                    "q8": {
                        "size": 3506762528,
                        "url": "chatllm_quantized_granite_3.0/granite-3-moe-3b.bin"
                    }
                }
            }
        }
    },
    "bce-emb": {
        "brief": "BCEmbedding: Bilingual and Cross-lingual Embedding for RAG.",
        "default": "0.2b",
        "license": "Apache License Version 2.0",
        "variants": {
            "0.2b": {
                "default": "q8",
                "quantized": {
                    "q8": {
                        "size": 299434560,
                        "url": "chatllm_quantized_models/bce_emb_q8.bin"
                    }
                }
            }
        }
    },
    "bce-reranker": {
        "brief": "BCEmbedding: Bilingual and Cross-lingual Reranker for RAG.",
        "default": "0.2b",
        "license": "Apache License Version 2.0",
        "variants": {
            "0.2b": {
                "default": "q8",
                "quantized": {
                    "q8": {
                        "size": 300065332,
                        "url": "chatllm_quantized_models/bce_reranker_q8.bin"
                    }
                }
            }
        }
    },
    "bge-m3": {
        "brief": "BGE-M3, distinguished for its versatility in Multi-Functionality, Multi-Linguality, and Multi-Granularity.",
        "default": "0.4b",
        "license": "MIT",
        "variants": {
            "0.4b": {
                "default": "q4_1",
                "quantized": {
                    "q4_1": {
                        "size": 359572992,
                        "url": "chatllm_quantized_models/bge-m3-q4_1.bin"
                    }
                }
            }
        }
    },
    "bge-reranker-m3": {
        "brief": "BGE-Reranker-v2-M3.",
        "default": "0.4b",
        "license": "MIT",
        "variants": {
            "0.4b": {
                "default": "q4_1",
                "quantized": {
                    "q4_1": {
                        "size": 360233284,
                        "url": "chatllm_quantized_models/bge-reranker-m3-q4_1.bin"
                    }
                }
            }
        }
    },
    "megrez" : {
        "brief": "Megrez-3B-Instruct is a large language model trained by Infinigence AI. Megrez-3B aims to provide a fast inference, compact, and powerful edge-side intelligent solution through software-hardware co-design.",
        "default": "3b",
        "license": "Apache License 2.0",
        "variants": {
            "3b": {
                "default": "q8",
                "quantized": {
                    "q8": {
                        "size": 3101520384,
                        "url": "chatllm_quantized_megrez/megrez-3b.bin"
                    }
                }
            }
        }
    },
    "falcon3" : {
        "brief": "Falcon3 family of Open Foundation Models is a set of pretrained and instruct LLMs ranging from 1B to 10B parameters.",
        "default": "1b",
        "license": "Apache License 2.0",
        "variants": {
            "1b": {
                "default": "q8",
                "quantized": {
                    "q8": {
                        "size": 1777274800,
                        "url": "chatllm_quantized_falcon3/falcon3-1b.bin"
                    }
                }
            },
            "3b": {
                "default": "q8",
                "quantized": {
                    "q8": {
                        "size": 3433097392,
                        "url": "chatllm_quantized_falcon3/falcon3-3b.bin"
                    }
                }
            }
        }
    },
    "exaone3.5": {
        "brief": "EXAONE 3.5, a collection of instruction-tuned bilingual (English and Korean) generative models ranging from 2.4B to 32B parameters, developed and released by LG AI Research.",
        "default": "2.4b",
        "license": "EXAONE AI Model License Agreement 1.1 - NC",
        "variants": {
            "2.4b": {
                "default": "q8",
                "quantized": {
                    "q8": {
                        "size": 2559193504,
                        "url": "chatllm_quantized_20241227/exaone-2.4b.bin"
                    }
                }
            }
        }
    },
    "telechat2": {
        "brief": "TeleChat2 is a large language model trained by the Artificial Intelligence Research Institute of China Telecom.",
        "default": "3b",
        "license": "Apache License 2.0",
        "variants": {
            "3b": {
                "default": "q8",
                "quantized": {
                    "q8": {
                        "size": 3265298768,
                        "url": "chatllm_quantized_20241227/telechat2-3b.bin"
                    }
                }
            }
        }
    },
    "alphageometry-lm": {
        "brief": "TeleChat2 is a large language model trained by the Artificial Intelligence Research Institute of China Telecom.",
        "default": "0.2b",
        "license": "Creative Commons Attribution 4.0 International (CC BY 4.0)",
        "variants": {
            "0.2b": {
                "default": "f32",
                "quantized": {
                    "f32": {
                        "size": 608298976,
                        "url": "chatllm_quantized_20241227/alphageometry-lm-f32.bin"
                    }
                }
            }
        }
    }
}